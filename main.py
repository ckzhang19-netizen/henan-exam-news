import requests
from bs4 import BeautifulSoup
import datetime
import os
import sys

# ç¯å¢ƒå˜é‡è·å– Token
TOKEN = os.environ.get("PUSHPLUS_TOKEN")
# å…³é”®è¯è®¾ç½® (ä¿è¯ä¿¡æ¯é«˜åº¦ç›¸å…³æ€§)
KEYWORDS = ["æ²³å—", "ä¸­è€ƒ", "é«˜è€ƒ", "æ‹›ç”Ÿ", "åˆ†æ•°çº¿", "å¿—æ„¿", "å½•å–", "æ”¿ç­–", "é€šçŸ¥", "å‘å¸ƒ", "æ”¹é©"]

def get_current_date():
    return datetime.datetime.now().strftime("%Y-%m-%d")

def filter_and_format(raw_links, source_name, base_url):
    """ç­›é€‰é“¾æ¥ï¼Œè¡¥å…¨URLå¹¶æ ¼å¼åŒ–"""
    results = []
    # å¢åŠ ä¸€ä¸ªå·²å¤„ç†é“¾æ¥é›†åˆï¼Œé˜²æ­¢é‡å¤
    processed_links = set() 
    
    for link in raw_links:
        text = link.get_text(strip=True)
        href = link.get('href')
        
        if not text or not href: continue
        
        # å…³é”®è¯è¿‡æ»¤ï¼Œå¹¶æ’é™¤çº¯ç²¹çš„å¯¼èˆª/çŸ­æ ‡é¢˜
        if not any(k in text for k in KEYWORDS) or len(text) < 5: 
             continue

        # è¡¥å…¨ç›¸å¯¹è·¯å¾„
        if href.startswith('/'): full_link = f"{base_url}{href}"
        elif href.startswith('http'): full_link = href
        else: continue
        
        # ç¡®ä¿é“¾æ¥ä¸é‡å¤ä¸”æœªè¢«å¤„ç†è¿‡
        if full_link not in processed_links:
            processed_links.add(full_link)
            results.append({'title': text, 'url': full_link, 'source': source_name})
            
    # åˆ—è¡¨é¡µé€šå¸¸æŒ‰æ—¶é—´å€’åºæ’åˆ—ï¼Œæˆ‘ä»¬å–å‰ 20 æ¡ï¼Œç¡®ä¿è¦†ç›–è¿‘åæ—¥å†…çš„é‡è¦èµ„è®¯
    return results[:20] 

def fetch_haeea():
    """æŠ“å–æ²³å—çœæ•™è‚²è€ƒè¯•é™¢"""
    print("æ­£åœ¨æŠ“å–ï¼šæ²³å—çœæ•™è‚²è€ƒè¯•é™¢...")
    url = "http://www.haeea.cn/a/zkss/" 
    base_url = "http://www.haeea.cn"
    
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        list_div = soup.find('div', class_='mainlist') or soup.find('div', class_='listcontent')
        links = list_div.find_all('a') if list_div else soup.find_all('a')
        print("DEBUG RAW LINKS:", [l.get('href') for l in links[:5]])
        return filter_and_format(links, 'è€ƒè¯•é™¢', base_url)
    except Exception as e:
        print(f"Error: è€ƒè¯•é™¢æŠ“å–å‡ºé”™: {e}")
        return []

def fetch_jyt():
    """æŠ“å–æ²³å—çœæ•™è‚²å…"""
    print("æ­£åœ¨æŠ“å–ï¼šæ²³å—çœæ•™è‚²å…...")
    url = "http://jyt.henan.gov.cn/xwdt/jytz/" 
    base_url = "http://jyt.henan.gov.cn"
    
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        list_ul = soup.find('ul', class_='list-con') 
        links = list_ul.find_all('a') if list_ul else soup.find_all('a') 
            
        return filter_and_format(links, 'æ•™è‚²å…', base_url)
    except Exception as e:
        print(f"Error: æ•™è‚²å…æŠ“å–å‡ºé”™: {e}")
        return []

def fetch_chsi():
    """æŠ“å–é˜³å…‰é«˜è€ƒä¿¡æ¯å¹³å°"""
    print("æ­£åœ¨æŠ“å–ï¼šé˜³å…‰é«˜è€ƒä¿¡æ¯å¹³å°...")
    # ç›®æ ‡ï¼šé˜³å…‰é«˜è€ƒ æ™®é€šé«˜æ ¡æ‹›ç”Ÿç›¸å…³æ–°é—»åˆ—è¡¨
    url = "https://gaokao.chsi.com.cn/gkxx/newsshow/" 
    base_url = "https://gaokao.chsi.com.cn"
    
    # å¢åŠ å…³é”®è¯ "æ²³å—" åˆ°æ ‡é¢˜ä¸­ï¼Œä»¥ç¡®ä¿å…¨å›½æ€§æ”¿ç­–ä¹Ÿä¸æ²³å—æœ‰å…³è”
    local_keywords = KEYWORDS + ["æ²³å—"] 

    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # æŸ¥æ‰¾ä¸»è¦æ–°é—»åˆ—è¡¨åŒºåŸŸ
        news_list = soup.find('ul', class_='news-list') 
        links = news_list.find_all('a') if news_list else soup.find_all('a')

        # ä½¿ç”¨æ›´ä¸¥æ ¼çš„è¿‡æ»¤ï¼Œåªä¿ç•™åŒ…å« "æ²³å—" å…³é”®è¯çš„å…¨å›½æ€§æ”¿ç­–
        results = []
        for item in filter_and_format(links, 'é˜³å…‰é«˜è€ƒ', base_url):
            if any(k in item['title'] for k in local_keywords):
                 results.append(item)
        return results
        
    except Exception as e:
        print(f"Error: é˜³å…‰é«˜è€ƒæŠ“å–å‡ºé”™: {e}")
        return []

def send_push(content):
    """å‘é€åˆ°å¾®ä¿¡ (é²æ£’ç‰ˆæœ¬)"""
    if not TOKEN: sys.exit(1)
    url = 'http://www.pushplus.plus/send'
    title = f"æ²³å—æ‹›è€ƒæ—¥æŠ¥ ({get_current_date()}) - ä¸‰æºè¿½è¸ª"
    
    data = {"token": TOKEN, "title": title, "content": content, "template": "markdown"}
    
    try:
        resp = requests.post(url, json=data, timeout=15)
        resp.raise_for_status() 
        # print(f"æ¨é€ç»“æœ: {resp.text}") # è°ƒè¯•ä¿¡æ¯
    except requests.exceptions.RequestException as e:
        print(f"å‘é€å¾®ä¿¡å¤±è´¥ï¼Œè¿æ¥é”™è¯¯æˆ–è¶…æ—¶ã€‚é”™è¯¯ä¿¡æ¯: {e}")

def main():
    # 1. è·å–æ‰€æœ‰æ•°æ®
    news_haeea = fetch_haeea()
    news_jyt = fetch_jyt()
    news_chsi = fetch_chsi()
    
    all_news = news_haeea + news_jyt + news_chsi
    
    # 2. æ•´åˆå†…å®¹
    if not all_news:
        send_push("ä»Šæ—¥æœªæœé›†åˆ°æ–°çš„æ²³å—æ‹›è€ƒç›¸å…³èµ„è®¯ (ä¸‰æºæ£€æŸ¥)ã€‚")
        return

    msg = [f"## ğŸ“… {get_current_date()} æ²³å—æ‹›è€ƒèµ„è®¯ (è¿‘åæ—¥)", "---"]
    
    # æŒ‰æ¥æºåˆ†ç»„å±•ç¤º
    if news_haeea:
        msg.append("### ğŸ›ï¸ æ²³å—çœæ•™è‚²è€ƒè¯•é™¢ (HEEA)")
        for item in news_haeea:
            msg.append(f"- [{item['title']}]({item['url']})")
        msg.append("\n") 
        
    if news_jyt:
        msg.append("### ğŸ“š æ²³å—çœæ•™è‚²å… (JYT)")
        for item in news_jyt:
            msg.append(f"- [{item['title']}]({item['url']})")
        msg.append("\n")

    if news_chsi:
        msg.append("### â˜€ï¸ é˜³å…‰é«˜è€ƒ (CHSI)")
        for item in news_chsi:
            msg.append(f"- [{item['title']}]({item['url']})")
        msg.append("\n")

    msg.append("\n---")
    msg.append("*ğŸ” ä¸‰æºè¿½è¸ªï¼Œè¿‘åæ—¥é‡ç‚¹èµ„è®¯ã€‚è¯·ä»¥å®˜æ–¹å‘å¸ƒä¸ºå‡†ã€‚*")
    
    final_content = "\n".join(msg)
    
    # 3. å‘é€
    send_push(final_content)

if __name__ == "__main__":
    main()
